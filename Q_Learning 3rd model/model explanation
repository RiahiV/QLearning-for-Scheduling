########## model discrprtion
#### States:
# For the agent, the definition of the state will coincide with the jobs that have finished and the order in which they were done, that is, the sequence that has been
# built. This means that at the beginning of an episode, the state of the environment is empty, since no work has been added to the sequence; therefore, at the end of the
# episode the state of the environment will be given by all the sequenced works.
#### Actions: the taking of an action by the agent is equivalent to deciding in which position of the already constructed sequence the work will be introduced.
#### Rewards: As described, the state is represented by the sequence that has been built, an action constitutes inserting a new job in said sequence and taking into account that
# our final objective is to minimize the makespan; We define reward 1/ğ‘šğ‘ğ‘˜ğ‘’ğ‘ ğ‘ğ‘ğ‘›, note that the greater the makespan of the sequence, the lower the reward for the selected action.
# It should be noted then that the highest Q-value determines the best action in any state.

####### In this version, the initial solution is generated by another method of RL
