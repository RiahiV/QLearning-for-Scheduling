# QLearning-for-Scheduling
MDP modelling of scheduling problems and Q learning technique to solve them

First idea:
States: For the agent, the definition of the state will coincide with the jobs that have finished and the order in which they were done, that is, the sequence that has been built. This means that at the beginning of an episode, the state of the environment is empty, since no work has been added to the sequence; therefore, at the end of the episode the state of the environment will be given by all the sequenced works.
Actions: 
â€¢	Action 1: pick job 1
â€¢	Action 2: pick job 2
â€¢	Action 3: pick job 3
â€¢	â€¦
Rewards: As described, the state is represented by the sequence that has been built, an action constitutes inserting a new job in said sequence and taking into account that our final objective is to minimize the makespan; We define reward 1/ğ‘šğ‘ğ‘˜ğ‘’ğ‘ ğ‘ğ‘ğ‘›, note that the greater the makespan of the sequence, the lower the reward for the selected action. It should be noted then that the highest Q-value determines the best action in any state.
Example:
The problem solving process is illustrated below through a small example of only three jobs and two machines. 
	Machine 1	Machine 2	Total processing time
Job 1	6	1	7
Job 2	2	4	6
Job 3	3	2	5
Job 4	5	2	7
Job 5	4	6	10
Job 6	3	5	8

	Action 1: pick job 1
Action 2: pick job 2
Action 3: pick job 3
â€¦
Episode 1:
â€¢	The agent observes the state of the environment ğ‘  = {}.
â€¢	Evaluate the set of possible actions
â€¢	Suppose that a random action, in this case the option 4 ğ‘ = ğ½4:1.
â€¢	Perform the selected action: 
â€¢	ğ‘ â€² = {ğ½4}
â€¢	ğ‘Ÿ = 1â„makespan ğ‘  â€² = 1â„5 = 0.2
Iteration 2:
Action 1: pick job 1
Action 2: pick job 2
Action 3: pick job 3
â€¦

â€¢	ğ‘  = {ğ½4}, actions = {ğ½1: 2,ğ½2: 2, J3:2, ğ½5:2, ğ½6:2}.
â€¢	Suppose that a random action, in this case the action 3.
â€¢	Perform the selected action:
ğ‘ â€² = {ğ½4, ğ½3} ğ‘Ÿ = 1â„makespan ğ‘ â€² = 1â„10 = 0.1
Second Idea:
States: For the agent, the definition of the state will coincide with the jobs that have finished and the order in which they were done, that is, the sequence that has been built. This means that at the beginning of an episode, the state of the environment is empty, since no work has been added to the sequence; therefore, at the end of the episode the state of the environment will be given by all the sequenced works.
Actions: the taking of an action by the agent is equivalent to deciding in which position of the already constructed sequence the work will be introduced.
Rewards: As described, the state is represented by the sequence that has been built, an action constitutes inserting a new job in said sequence and taking into account that our final objective is to minimize the makespan; We define reward 1/ğ‘šğ‘ğ‘˜ğ‘’ğ‘ ğ‘ğ‘ğ‘›, note that the greater the makespan of the sequence, the lower the reward for the selected action. It should be noted then that the highest Q-value determines the best action in any state.
Example:
The problem solving process is illustrated below through a small example of only three jobs and two machines. 
	Machine 1	Machine 2	Total processing time
Job 1	6	1	7
Job 2	2	4	6
Job 3	3	2	5
Job 4	5	2	7
Job 5	4	6	10
Job 6	3	5	8

# initial sequence is 3,2,1,4,6,5
Episode 1:
â€¢	The agent observes the state of the environment ğ‘  = {}.
â€¢	Evaluate the set of possible actions
â€¢	Suppose that a random value less than Îµ is generated and therefore a random action, in this case there is only the option ğ‘ = ğ½3:1.
â€¢	Perform the selected action: 
â€¢	ğ‘ â€² = {ğ½3}
â€¢	ğ‘Ÿ = 1â„makespan ğ‘  â€² = 1â„8 = 0.125
Iteration 2:
â€¢	ğ‘  = {ğ½3}, actions = {ğ½2: 1,ğ½2: 2}.
â€¢	Suppose that a random value less than Îµ is generated and therefore a random action, in this case the option 1.
â€¢	Perform the selected action:
ğ‘ â€² = {ğ½2, ğ½3} ğ‘Ÿ = 1â„makespan ğ‘ â€² = 1â„10 = 0.1
Iteration 3:
â€¢	ğ‘  = { ğ½2, ğ½3}, actions = {ğ½1: 1,ğ½1: 2, ğ½1: 3}.
â€¢	Suppose that a random value less than Îµ is generated and therefore a random action, in this case the option 2.
â€¢	Perform the selected action:
ğ‘ â€² = {ğ½2, J1, ğ½3} ğ‘Ÿ = 1â„makespan ğ‘ â€² = 1â„20 = 0.05


